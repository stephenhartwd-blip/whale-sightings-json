#!/usr/bin/env python3
"""
Update whale_sightings.json from curated sources.

Deterministic:
- Does NOT "search the web"
- Polls a curated list of sources and parses them

This version supports:
- multiple sources in config/sources.yml
- generic HTML log parsing (date headings + species keywords)
- basic article parsing (single dated post)
- target_total + species_targets selection (best effort)
"""

from __future__ import annotations

import json
import math
import os
import re
import sys
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import requests
import yaml
from bs4 import BeautifulSoup
from dateutil import tz, parser as dateparser

ALLOWED_SPECIES = {"Orca", "Humpback", "Sperm whale", "Great White Shark", "Blue Whale"}


# -----------------------
# Helpers
# -----------------------

def now_local(tz_name: str) -> datetime:
    return datetime.now(tz.gettz(tz_name))


def to_date_str(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%d")


def safe_float(x: Any, default: float) -> float:
    try:
        return float(x)
    except Exception:
        return default


def clamp_nudge(lat: float, lon: float, dlat: float, dlon: float) -> Tuple[float, float]:
    """Apply at most 0.05 degrees movement per component."""
    dlat = max(-0.05, min(0.05, dlat))
    dlon = max(-0.05, min(0.05, dlon))
    return lat + dlat, lon + dlon


def slugify(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9]+", "-", s)
    s = re.sub(r"-+", "-", s).strip("-")
    return s


def species_key(species: str) -> str:
    return {
        "Orca": "orca",
        "Humpback": "humpback",
        "Sperm whale": "sperm",
        "Great White Shark": "white",
        "Blue Whale": "blue",
    }[species]


def normalize_area(area: str) -> str:
    a = (area or "").strip()
    if not a:
        return "Unknown (offshore)"
    # avoid double "(offshore)"
    if "offshore" in a.lower():
        return a
    return f"{a} (offshore)"


def infer_behaviors(text: str) -> List[str]:
    t = (text or "").lower()
    out = ["reported"]
    if any(k in t for k in ["hunt", "predation", "prey", "kill"]):
        out.append("hunting")
    if any(k in t for k in ["feed", "feeding", "lunge", "krill"]):
        out.append("feeding")
    if any(k in t for k in ["breach", "breaching"]):
        out.append("breaching")
    if any(k in t for k in ["call", "hydrophone", "vocal"]):
        out.append("vocalizing")
    if any(k in t for k in ["travel", "headed", "northbound", "southbound", "migrat"]):
        out.append("traveling")

    # de-dupe preserving order
    seen = set()
    deduped: List[str] = []
    for b in out:
        if b not in seen:
            deduped.append(b)
            seen.add(b)
    return deduped


def infer_species_hits(text: str, force_species: Optional[str] = None) -> List[str]:
    """
    Return list of allowed species found in text.
    If force_species is provided, returns [force_species] if allowed.
    """
    if force_species:
        fs = force_species.strip()
        return [fs] if fs in ALLOWED_SPECIES else []

    t = (text or "")

    hits: List[str] = []
    if re.search(r"\b(orcas?|killer whale|bigg'?s|southern resident)\b", t, re.IGNORECASE):
        hits.append("Orca")
    if re.search(r"\bhumpback\b", t, re.IGNORECASE):
        hits.append("Humpback")
    if re.search(r"\bsperm whale\b|\bsperm\b", t, re.IGNORECASE):
        hits.append("Sperm whale")
    if re.search(r"\bblue whale\b", t, re.IGNORECASE):
        hits.append("Blue Whale")
    if re.search(r"\bgreat white\b|\bwhite shark\b", t, re.IGNORECASE):
        hits.append("Great White Shark")

    # de-dupe preserving order
    out: List[str] = []
    seen = set()
    for h in hits:
        if h not in seen and h in ALLOWED_SPECIES:
            out.append(h)
            seen.add(h)
    return out


def parse_date_any(s: Optional[str], tz_name: str) -> Optional[datetime]:
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt is None:
            return None
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=tz.gettz(tz_name))
        else:
            dt = dt.astimezone(tz.gettz(tz_name))
        return dt
    except Exception:
        return None


def infer_date_from_url(url: str, tz_name: str) -> Optional[datetime]:
    # Matches /2026/1/14/ or /2026/01/14/ or 2026-01-14
    m = re.search(r"/(20\d{2})/(\d{1,2})/(\d{1,2})(?:/|$)", url)
    if m:
        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
        return datetime(y, mo, d, tzinfo=tz.gettz(tz_name))
    m = re.search(r"(20\d{2})-(\d{2})-(\d{2})", url)
    if m:
        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
        return datetime(y, mo, d, tzinfo=tz.gettz(tz_name))
    return None


# -----------------------
# Data model
# -----------------------

@dataclass(frozen=True)
class Candidate:
    date: datetime
    species: str
    name: str
    info: str
    area: str
    source: str
    latitude: float
    longitude: float
    behaviors: List[str]

    def dedupe_key(self) -> Tuple[str, str, str, str, float, float]:
        return (
            to_date_str(self.date),
            self.species,
            self.source,
            self.area,
            round(self.latitude, 4),
            round(self.longitude, 4),
        )


# -----------------------
# Parsers
# -----------------------

def parse_orcanetwork_recent_sightings(cfg: Dict[str, Any], session: requests.Session, tz_name: str) -> List[Candidate]:
    url = cfg["url"]
    html = session.get(url, timeout=30).text
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n")

    month_re = r"(January|February|March|April|May|June|July|August|September|October|November|December)"
    heading_pat = re.compile(rf"^(?P<month>{month_re})\s+(?P<day>\d{{1,2}})\s*$", re.MULTILINE)

    matches = list(heading_pat.finditer(text))
    if not matches:
        return []

    today = now_local(tz_name)
    year = today.year

    # fallback offshore coords (safe-ish)
    fallback_lat = safe_float(cfg.get("latitude"), 48.45)
    fallback_lon = safe_float(cfg.get("longitude"), -123.05)
    area = normalize_area(str(cfg.get("area", "Salish Sea")))

    out: List[Candidate] = []

    for i, m in enumerate(matches):
        start = m.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        block = text[start:end].strip()

        month_name = m.group("month")
        day = int(m.group("day"))
        dt = datetime.strptime(f"{year} {month_name} {day}", "%Y %B %d").replace(tzinfo=tz.gettz(tz_name))
        if dt.date() > today.date():
            dt = dt.replace(year=year - 1)

        species_hits = infer_species_hits(block)
        for sp in species_hits:
            lat, lon = fallback_lat, fallback_lon
            if "uncertain_offshore_nudge" in cfg:
                nud = cfg["uncertain_offshore_nudge"]
                lat, lon = clamp_nudge(lat, lon, safe_float(nud.get("dlat"), 0.0), safe_float(nud.get("dlon"), 0.0))

            name = f"{sp} sighting ({area})"
            info = f"Orca Network recent sightings lists a {sp.lower()} report for {area} on {to_date_str(dt)}."
            out.append(
                Candidate(
                    date=dt,
                    species=sp,
                    name=name,
                    info=info,
                    area=area,
                    source=url,
                    latitude=float(lat),
                    longitude=float(lon),
                    behaviors=infer_behaviors(block),
                )
            )

    max_items = int(cfg.get("max_items", 9999))
    out.sort(key=lambda c: (c.date, c.species), reverse=True)
    return out[:max_items]


def _extract_date_blocks(text: str) -> List[Tuple[datetime, str]]:
    """
    Given large text, try to split into dated blocks.
    Returns list of (date, block_text).
    """
    month_re = r"(January|February|March|April|May|June|July|August|September|October|November|December)"
    dow_re = r"(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)"
    # patterns like "Wednesday 14 January 2026" or "January 14, 2026" etc.
    pat1 = re.compile(rf"^(?:{dow_re}\s*)?(?P<month>{month_re})\s+(?P<day>\d{{1,2}})(?:,\s*(?P<year>20\d{{2}}))?\s*$", re.MULTILINE)
    pat2 = re.compile(rf"^(?:{dow_re}\s*)?(?P<day>\d{{1,2}})\s+(?P<month>{month_re})(?:\s+(?P<year>20\d{{2}}))?\s*$", re.MULTILINE)
    pat3 = re.compile(rr"^(?P<year>20\d{2})[-/](?P<month>\d{1,2})[-/](?P<day>\d{1,2})\s*$", re.MULTILINE)

    matches = []
    matches.extend([(m.start(), m.end(), "m1", m.groupdict()) for m in pat1.finditer(text)])
    matches.extend([(m.start(), m.end(), "m2", m.groupdict()) for m in pat2.finditer(text)])
    matches.extend([(m.start(), m.end(), "m3", m.groupdict()) for m in pat3.finditer(text)])

    matches.sort(key=lambda x: x[0])
    if not matches:
        return []

    # build date objs and blocks
    out: List[Tuple[datetime, str]] = []
    for i, (s, e, kind, gd) in enumerate(matches):
        block_start = e
        block_end = matches[i + 1][0] if i + 1 < len(matches) else len(text)
        block = text[block_start:block_end].strip()

        # parse date
        if kind == "m3":
            y = int(gd["year"]); mo = int(gd["month"]); d = int(gd["day"])
            dt = datetime(y, mo, d)
        else:
            year = int(gd.get("year") or datetime.now().year)
            month_name = gd["month"]
            day = int(gd["day"])
            dt = datetime.strptime(f"{year} {month_name} {day}", "%Y %B %d")

        out.append((dt, block))
    return out


def parse_generic_html_log(cfg: Dict[str, Any], session: requests.Session, tz_name: str) -> List[Candidate]:
    """
    Generic parser for "daily report / whale diary / sightings log" pages.
    - Splits page text into blocks by date headings where possible
    - Otherwise tries to infer date from URL and creates one block from the page
    """
    url = cfg["url"]
    html = session.get(url, timeout=30).text

    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    text = soup.get_text("\n")
    text = re.sub(r"[ \t]+", " ", text)

    fallback_lat = safe_float(cfg.get("latitude"), 0.0)
    fallback_lon = safe_float(cfg.get("longitude"), 0.0)
    area = normalize_area(str(cfg.get("area", "Unknown")))
    force_species = cfg.get("force_species")  # optional
    max_items = int(cfg.get("max_items", 9999))

    blocks = _extract_date_blocks(text)

    # If no dated blocks, fall back to a single entry dated from URL or "today"
    if not blocks:
        dt = infer_date_from_url(url, tz_name) or now_local(tz_name)
        blocks = [(dt.replace(tzinfo=None), text)]

    today = now_local(tz_name).date()
    out: List[Candidate] = []

    for dt_naive, block in blocks:
        # attach tz + handle year boundary (avoid future dates)
        dt = dt_naive.replace(tzinfo=tz.gettz(tz_name))
        if dt.date() > today:
            dt = dt.replace(year=dt.year - 1)

        species_hits = infer_species_hits(block, force_species=force_species)

        # If still none, skip this date block
        if not species_hits:
            continue

        for sp in species_hits:
            lat, lon = fallback_lat, fallback_lon
            # nudge if configured
            if "uncertain_offshore_nudge" in cfg:
                nud = cfg["uncertain_offshore_nudge"]
                lat, lon = clamp_nudge(lat, lon, safe_float(nud.get("dlat"), 0.0), safe_float(nud.get("dlon"), 0.0))

            name = f"{sp} sighting ({area})"
            info = f"Source log lists a {sp.lower()} report for {area} on {to_date_str(dt)}."

            out.append(
                Candidate(
                    date=dt,
                    species=sp,
                    name=name,
                    info=info,
                    area=area,
                    source=url,
                    latitude=float(lat),
                    longitude=float(lon),
                    behaviors=infer_behaviors(block),
                )
            )

    out.sort(key=lambda c: (c.date, c.species, c.source), reverse=True)
    return out[:max_items]


def parse_article_basic(cfg: Dict[str, Any], session: requests.Session, tz_name: str) -> List[Candidate]:
    """
    Basic single-article parser:
    - date from URL if present, else from meta tags if possible, else today
    - species inferred from article text
    """
    url = cfg["url"]
    html = session.get(url, timeout=30).text
    soup = BeautifulSoup(html, "html.parser")

    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    text = soup.get_text("\n")
    text = re.sub(r"[ \t]+", " ", text).strip()

    area = normalize_area(str(cfg.get("area", "Unknown")))
    fallback_lat = safe_float(cfg.get("latitude"), 0.0)
    fallback_lon = safe_float(cfg.get("longitude"), 0.0)
    max_items = int(cfg.get("max_items", 2))

    dt = infer_date_from_url(url, tz_name)
    if dt is None:
        dt = now_local(tz_name)

    species_hits = infer_species_hits(text, force_species=cfg.get("force_species"))
    if not species_hits:
        return []

    out: List[Candidate] = []
    for sp in species_hits[:2]:
        lat, lon = fallback_lat, fallback_lon
        if "uncertain_offshore_nudge" in cfg:
            nud = cfg["uncertain_offshore_nudge"]
            lat, lon = clamp_nudge(lat, lon, safe_float(nud.get("dlat"), 0.0), safe_float(nud.get("dlon"), 0.0))

        name = f"{sp} sighting ({area})"
        info = f"Article mentions {sp.lower()} activity near {area} on {to_date_str(dt)}."
        out.append(
            Candidate(
                date=dt,
                species=sp,
                name=name,
                info=info,
                area=area,
                source=url,
                latitude=float(lat),
                longitude=float(lon),
                behaviors=infer_behaviors(text),
            )
        )

    out.sort(key=lambda c: c.date, reverse=True)
    return out[:max_items]


def parse_pirsa_shark_sightings(cfg: Dict[str, Any], session: requests.Session, tz_name: str) -> List[Candidate]:
    """
    PIRSA page is shark-focused; force Great White Shark unless text clearly indicates otherwise.
    """
    cfg2 = dict(cfg)
    cfg2["force_species"] = "Great White Shark"
    return parse_generic_html_log(cfg2, session, tz_name)


# Aliases: your sources.yml parser names -> actual functions
PARSERS = {
    "orcanetwork_recent_sightings": parse_orcanetwork_recent_sightings,

    "eaglewing_daily_sighting_report": parse_generic_html_log,
    "victoriawhalewatching_captains_log": parse_article_basic,

    "montereybay_sightings": parse_generic_html_log,
    "dolphinsafari_sightings_log": parse_generic_html_log,
    "hawaiianadventures_weekly_whale_report": parse_generic_html_log,

    "whalewatchingakureyri_blog": parse_article_basic,
    "elding_whale_diary": parse_generic_html_log,
    "seatrips_whale_diary": parse_generic_html_log,

    "whalewatchwesternaustralia_daily": parse_article_basic,
    "pirsa_shark_sightings": parse_pirsa_shark_sightings,

    "article_basic": parse_article_basic,
}


# -----------------------
# Config + selection
# -----------------------

def load_config(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def within_window(dt: datetime, now_dt: datetime, max_days: int) -> bool:
    return (now_dt.date() - dt.date()).days <= max_days


def is_recent(dt: datetime, now_dt: datetime, min_recent_days: int) -> bool:
    return (now_dt.date() - dt.date()).days <= min_recent_days


def target_counts_from_ratios(target_total: int, ratios: Dict[str, float]) -> Dict[str, int]:
    cleaned = {k: float(v) for k, v in (ratios or {}).items() if k in ALLOWED_SPECIES}
    if not cleaned:
        return {}

    counts: Dict[str, int] = {k: int(math.floor(target_total * cleaned[k])) for k in cleaned.keys()}
    used = sum(counts.values())
    remainder = target_total - used
    if remainder <= 0:
        return counts

    fracs = sorted(
        cleaned.keys(),
        key=lambda k: (target_total * cleaned[k] - math.floor(target_total * cleaned[k]), cleaned[k]),
        reverse=True,
    )
    idx = 0
    while remainder > 0 and fracs:
        k = fracs[idx % len(fracs)]
        counts[k] = counts.get(k, 0) + 1
        remainder -= 1
        idx += 1

    return counts


def select_candidates(
    candidates: List[Candidate],
    tz_name: str,
    target_total: int,
    species_targets: Dict[str, float],
    min_recent_days: int,
    min_recent_fraction: float,
) -> List[Candidate]:
    now_dt = now_local(tz_name)
    if target_total <= 0:
        return []

    # de-dupe
    deduped: Dict[Tuple[str, str, str, str, float, float], Candidate] = {}
    for c in candidates:
        deduped[c.dedupe_key()] = c
    candidates = list(deduped.values())
    candidates.sort(key=lambda c: (c.date, c.species, c.source, c.area, c.name), reverse=True)

    recent_pool = [c for c in candidates if is_recent(c.date, now_dt, min_recent_days)]
    old_pool = [c for c in candidates if not is_recent(c.date, now_dt, min_recent_days)]

    min_recent_count = int(math.ceil(target_total * float(min_recent_fraction)))
    min_recent_count = max(0, min(target_total, min_recent_count))

    def by_species(pool: List[Candidate]) -> Dict[str, List[Candidate]]:
        d: Dict[str, List[Candidate]] = {s: [] for s in ALLOWED_SPECIES}
        for c in pool:
            if c.species in d:
                d[c.species].append(c)
        for s in d:
            d[s].sort(key=lambda c: (c.date, c.source, c.area, c.name), reverse=True)
        return d

    recent_by = by_species(recent_pool)
    all_by = by_species(recent_pool + old_pool)

    picked: List[Candidate] = []
    picked_counts: Dict[str, int] = {s: 0 for s in ALLOWED_SPECIES}

    def pop_n(pools: Dict[str, List[Candidate]], sp: str, n: int) -> List[Candidate]:
        if n <= 0:
            return []
        take = pools.get(sp, [])[:n]
        pools[sp] = pools.get(sp, [])[n:]
        return take

    # Step 1: meet minimum recent quota
    if min_recent_count > 0:
        recent_targets = target_counts_from_ratios(min_recent_count, species_targets)
        for sp, need in recent_targets.items():
            for c in pop_n(recent_by, sp, need):
                picked.append(c); picked_counts[sp] += 1

        while len(picked) < min_recent_count:
            next_c = None
            next_sp = None
            for sp in ALLOWED_SPECIES:
                if recent_by.get(sp):
                    cand = recent_by[sp][0]
                    if next_c is None or cand.date > next_c.date:
                        next_c = cand; next_sp = sp
            if next_c is None or next_sp is None:
                break
            recent_by[next_sp] = recent_by[next_sp][1:]
            picked.append(next_c); picked_counts[next_sp] += 1

    # Step 2: fill to target_total by ratios
    total_targets = target_counts_from_ratios(target_total, species_targets)
    needed: Dict[str, int] = {}
    for sp, tgt in total_targets.items():
        needed[sp] = max(0, int(tgt) - int(picked_counts.get(sp, 0)))

    for sp, need in needed.items():
        for c in pop_n(all_by, sp, need):
            picked.append(c); picked_counts[sp] += 1

    while len(picked) < target_total:
        next_c = None
        next_sp = None
        for sp in ALLOWED_SPECIES:
            if all_by.get(sp):
                cand = all_by[sp][0]
                if next_c is None or cand.date > next_c.date:
                    next_c = cand; next_sp = sp
        if next_c is None or next_sp is None:
            break
        all_by[next_sp] = all_by[next_sp][1:]
        picked.append(next_c); picked_counts[next_sp] += 1

    return picked[:target_total]


def build_entries(candidates: List[Candidate]) -> List[Dict[str, Any]]:
    candidates = sorted(candidates, key=lambda c: (c.date, c.species, c.source, c.area, c.name), reverse=True)

    entries: List[Dict[str, Any]] = []
    counters: Dict[Tuple[str, str], int] = {}

    for c in candidates:
        date_str = to_date_str(c.date)
        region_base = slugify(c.area.split("(")[0])[:20]
        region = region_base if region_base else "region"
        skey = species_key(c.species)
        k = (date_str, skey)
        counters[k] = counters.get(k, 0) + 1
        idx = counters[k]
        entry_id = f"{date_str}-{region}-{skey}-{idx:02d}"

        # Field order EXACTLY as your Swift model expects
        entries.append(
            {
                "id": entry_id,
                "name": c.name,
                "species": c.species,
                "info": c.info,
                "date": date_str,
                "latitude": float(c.latitude),
                "longitude": float(c.longitude),
                "area": c.area,
                "source": c.source,
                "behaviors": c.behaviors,
            }
        )

    entries.sort(key=lambda e: e["date"], reverse=True)
    return entries


# -----------------------
# Main
# -----------------------

def main() -> None:
    root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    cfg = load_config(os.path.join(root, "config", "sources.yml"))

    tz_name = cfg.get("timezone", "America/Vancouver")
    now_dt = now_local(tz_name)

    max_days = int(cfg.get("max_days", 14))
    min_recent_days = int(cfg.get("min_recent_days", 7))
    min_recent_fraction = float(cfg.get("min_recent_fraction", 0.50))

    target_total = int(cfg.get("target_total", 80))
    species_targets = cfg.get("species_targets", {}) or {}

    session = requests.Session()
    session.headers.update(
        {
            "User-Agent": "WhaleSightingsBot/1.2 (GitHub Actions)",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        }
    )

    all_candidates: List[Candidate] = []

    sources = cfg.get("sources", []) or []
    print(f"Loaded {len(sources)} sources from config.")

    for s in sources:
        key = str(s.get("key", "source"))
        url = str(s.get("url", "")).strip()
        parser_name = s.get("parser")

        if not url:
            print(f"SKIP: {key} missing url")
            continue
        if not parser_name or parser_name not in PARSERS:
            print(f"SKIP: {key} unknown parser '{parser_name}' (url={url})")
            continue

        try:
            items = PARSERS[parser_name](s, session, tz_name)
            print(f"SRC: {key} ({parser_name}) -> {len(items)} candidates")
            all_candidates.extend(items)
        except Exception as e:
            print(f"WARN: source {key} failed: {e}")

    windowed = [c for c in all_candidates if within_window(c.date, now_dt, max_days)]
    print(f"Window filter: {len(all_candidates)} -> {len(windowed)} within last {max_days} days")

    selected = select_candidates(
        windowed,
        tz_name=tz_name,
        target_total=target_total,
        species_targets=species_targets,
        min_recent_days=min_recent_days,
        min_recent_fraction=min_recent_fraction,
    )

    if len(selected) < target_total:
        print(f"WARN: Only {len(selected)} entries available (target_total={target_total}). Add more sources or increase max_days.")

    entries = build_entries(selected)

    out_path = os.path.join(root, "whale_sightings.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(entries, f, ensure_ascii=False, indent=2)
        f.write("\n")

    # Stats
    by_species: Dict[str, int] = {}
    recent_count = 0
    for c in selected:
        by_species[c.species] = by_species.get(c.species, 0) + 1
        if is_recent(c.date, now_dt, min_recent_days):
            recent_count += 1

    print(f"Wrote {len(entries)} entries to whale_sightings.json")
    print(f"Recent (<= {min_recent_days}d): {recent_count}/{len(entries)}")
    print("Species counts:", dict(sorted(by_species.items(), key=lambda kv: kv[0])))


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"FATAL: {e}")
        sys.exit(1)
